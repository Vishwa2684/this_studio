{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://api.edamam.com/search?q=${prompt}&app_id=7aa516a5&app_key=dc836a223fb788b11ae390504d9e97ce&from=0&to=10\"\"\"\n",
    "import torch\n",
    "import accelerate\n",
    "from dataclasses import fields,dataclass\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig,prepare_model_for_kbit_training\n",
    "from transformers import (AutoTokenizer, \n",
    "                        AutoModelForCausalLM,\n",
    "                        pipeline,\n",
    "                        BitsAndBytesConfig,\n",
    "                        GemmaTokenizer,\n",
    "                        HfArgumentParser,\n",
    "                        TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dataset = load_dataset('Amod/mental_health_counseling_conversations',split='train')\n",
    "# dataset\n",
    "# df = pd.DataFrame(dataset)\n",
    "df = pd.read_csv('./modify2.csv')\n",
    "# 13 is threshold for this dataset\n",
    "# df = df[:12]\n",
    "# Split the data into training and evaluation sets\n",
    "\n",
    "df.head()\n",
    "df.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rows(row):\n",
    "    q=row['Input']\n",
    "    a=row['Output']\n",
    "    # Formatting our dataset to LLaMa format\n",
    "    format = f'[INST] {q} [/INST] {a}'\n",
    "    return format\n",
    "\n",
    "df['formatted'] = df.apply(format_rows,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.rename(columns={'formatted':'text'})\n",
    "new_df = new_df[['text']]\n",
    "new_df.head()\n",
    "train_df, eval_df = train_test_split(new_df, test_size=0.26, random_state=42)\n",
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the splits to CSV files if needed\n",
    "train_df.to_csv('./train.csv', index=False)\n",
    "eval_df.to_csv('./eval.csv', index=False)\n",
    "\n",
    "training_ds = load_dataset('csv', data_files='./train.csv', split='train')\n",
    "evaluation_ds = load_dataset('csv', data_files='./eval.csv', split='train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"microsoft/phi-2\"\n",
    "# new_model = 'Recipe-Generator'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast =True)\n",
    "\n",
    "# tokenizer.pad_token =tokenizer.eos_token\n",
    "\n",
    "# tokenizer.padding_side = 'right'\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4', #normalising float 4\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     # Double quantization can degrade the  performance\n",
    "#     bnb_4bit_use_double_quant=False\n",
    "# )\n",
    "# try:\n",
    "\n",
    "#     model =AutoModelForCausalLM.from_pretrained(\n",
    "#                                         model_id,\n",
    "#                                         trust_remote_code = True,\n",
    "#                                         quantization_config = bnb_config,\n",
    "#                                         flash_attn=True,\n",
    "#                                         flash_rotary = True,\n",
    "#                                         # flash_dense = True,\n",
    "#                                         low_cpu_mem_usage=True,\n",
    "#                                         device_map = {\"\":0},\n",
    "#                                         revision='refs/pr/23'\n",
    "#                                         )\n",
    "\n",
    "# # Welp try this in runpod.ai bc my CUDA memory is running out\n",
    "#     model.config.use_cache = False\n",
    "#     model.config.pretraining_tp = 1\n",
    "#     model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir = './Recipe-Generator',\n",
    "#         num_train_epochs=2,\n",
    "#         per_device_train_batch_size=2,\n",
    "#         gradient_accumulation_steps=32,\n",
    "#         eval_strategy = 'steps',\n",
    "#         eval_steps = 1500,\n",
    "#         optim = 'paged_adamw_8bit',\n",
    "#         learning_rate = 2e-4,\n",
    "#         lr_scheduler_type='cosine',\n",
    "#         save_steps = 1500,\n",
    "#         warmup_ratio=0.05,\n",
    "#         weight_decay=0.01,\n",
    "#         max_steps=-1\n",
    "#     )\n",
    "\n",
    "#     peft_config = LoraConfig(\n",
    "#         r=32,\n",
    "#         lora_alpha=64,\n",
    "#         lora_dropout = 0.05,\n",
    "#         bias='none',\n",
    "#         task_type='CAUSAL_LM',\n",
    "#         target_modules=['Wqkv','fc1','fc2']\n",
    "#     )\n",
    "\n",
    "#     trainer = SFTTrainer(\n",
    "#         model = model,\n",
    "#         train_dataset=training_ds,\n",
    "#         eval_dataset = evaluation_ds,\n",
    "#         peft_config=peft_config,\n",
    "#         dataset_text_field='text',\n",
    "#         tokenizer=tokenizer,\n",
    "#         args=training_args\n",
    "#     )\n",
    "#     trainer.train()\n",
    "#     trainer.save_model('./Recipe-Generator')\n",
    "#     tokenizer.save_pretrained('./Recipe-Generator')\n",
    "# except Exception as e:\n",
    "#     print('At line:',e.__traceback__.tb_lineno)\n",
    "#     print('________________ERROR________________:',e)\n",
    "\n",
    "\n",
    "from transformers import IntervalStrategy\n",
    "\n",
    "model_id = \"microsoft/phi-2\"\n",
    "new_model = 'Recipe-Generator'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# BitsAndBytes configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',  # normalizing float 4\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False  # Avoid double quantization for better performance\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        flash_attn=True,\n",
    "        flash_rotary=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map={\"\": 0},\n",
    "        revision='refs/pr/23'\n",
    "    )\n",
    "\n",
    "    # Set model configuration for training\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./Recipe-Generator',\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,  # Reduce batch size to fit in memory\n",
    "        gradient_accumulation_steps=64,  # Increase gradient accumulation steps\n",
    "        eval_strategy=IntervalStrategy.STEPS,\n",
    "        eval_steps=1500,\n",
    "        optim='paged_adamw_8bit',\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type='cosine',\n",
    "        save_steps=1500,\n",
    "        warmup_ratio=0.05,\n",
    "        weight_decay=0.01,\n",
    "        fp16=True,  # Use mixed precision\n",
    "        max_steps=-1\n",
    "    )\n",
    "\n",
    "    # PEFT configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        bias='none',\n",
    "        task_type='CAUSAL_LM',\n",
    "        target_modules=['Wqkv', 'fc1', 'fc2']\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=training_ds,\n",
    "        eval_dataset=evaluation_ds,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field='text',\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "    # Train and save the model\n",
    "    trainer.train()\n",
    "    trainer.save_model('./Recipe-Generator')\n",
    "    tokenizer.save_pretrained('./Recipe-Generator')\n",
    "\n",
    "except Exception as e:\n",
    "    print('At line:', e.__traceback__.tb_lineno)\n",
    "    print('________________ERROR________________:', e)\n",
    "\n",
    "\n",
    "# # Adjust max sequence length in tokenization\n",
    "# max_sequence_length = 690\n",
    "# def preprocess_example(example):\n",
    "#     example['text'] = example['text'][:max_sequence_length]\n",
    "#     return example\n",
    "# training_ds = training_ds.map(preprocess_example)\n",
    "\n",
    "# model_id = \"microsoft/phi-2\"\n",
    "# new_model = 'mental-health-LLM'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = 'right'\n",
    "\n",
    "# try:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         trust_remote_code=True,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         device_map={\"\": 0},\n",
    "#         revision='refs/pr/23'\n",
    "#     )\n",
    "\n",
    "#     model.config.use_cache = False\n",
    "#     model.config.pretraining_tp = 1\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir='./mental-healthLLM',\n",
    "#         num_train_epochs=2,\n",
    "#         per_device_train_batch_size=2,  # Reduced batch size for T4\n",
    "#         gradient_accumulation_steps=32,  # Increased to maintain effective batch size\n",
    "#         evaluation_strategy='steps',\n",
    "#         eval_steps=1500,\n",
    "#         optim='paged_adamw_8bit',\n",
    "#         learning_rate=2e-4,\n",
    "#         lr_scheduler_type='cosine',\n",
    "#         save_steps=1500,\n",
    "#         warmup_ratio=0.05,\n",
    "#         weight_decay=0.01,\n",
    "#         max_steps=-1,\n",
    "#         fp16=True  # Enable mixed precision to save memory\n",
    "#     )\n",
    "\n",
    "#     peft_config = LoraConfig(\n",
    "#         r=32,\n",
    "#         lora_alpha=64,\n",
    "#         lora_dropout=0.05,\n",
    "#         bias='none',\n",
    "#         task_type='CAUSAL_LM',\n",
    "#         target_modules=['Wqkv', 'fc1', 'fc2']\n",
    "#     )\n",
    "\n",
    "#     trainer = SFTTrainer(\n",
    "#         model=model,\n",
    "#         train_dataset=training_ds,\n",
    "#         peft_config=peft_config,\n",
    "#         dataset_text_field='text',  # Use the 'formatted' column\n",
    "#         tokenizer=tokenizer,\n",
    "#         args=training_args\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "#     # Save the fine-tuned model and tokenizer\n",
    "#     trainer.save_model('./mental-healthLLM')\n",
    "#     tokenizer.save_pretrained('./mental-healthLLM')\n",
    "\n",
    "# except Exception as e:\n",
    "#     print('At line:', e.__traceback__.tb_lineno)\n",
    "#     print('________________ERROR________________:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer, \n",
    "                        AutoModelForCausalLM,\n",
    "                        pipeline,\n",
    "                        BitsAndBytesConfig,\n",
    "                        GemmaTokenizer,\n",
    "                        HfArgumentParser,\n",
    "                        TrainingArguments)\n",
    "\n",
    "import torch\n",
    "import accelerate            \n",
    "try:\n",
    "   # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./Recipe-Generator')\n",
    "\n",
    "    # Load the fine-tuned model from the checkpoint\n",
    "    model = AutoModelForCausalLM.from_pretrained('./Recipe-Generator')\n",
    "\n",
    "    # Prepare the input text\n",
    "    input_text = \"how to make chicken 65 dish\"\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(input_ids, max_length=1024, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(generated_text)\n",
    "except Exception as e:\n",
    "    print('At line:', e.__traceback__.tb_lineno)\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
